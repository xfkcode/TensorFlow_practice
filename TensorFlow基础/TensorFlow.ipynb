{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow🌵\n",
    "`TensorFlow` 是采用数据流图 [***data/ flow/ graphs***] 来计算, 首先创建一个数据流流图, 然后再将数据（张量[***tensor***]）放在数据流图中计算.   \n",
    "节点（Nodes）在图中表示数学操作,图中的线（edges）则表示在节点间相互联系的多维数据数组, 即张量（tensor).  \n",
    "训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.03418568] [0.49445015]\n",
      "20 [0.06306686] [0.32119435]\n",
      "40 [0.08977877] [0.30586553]\n",
      "60 [0.09717127] [0.3016233]\n",
      "80 [0.09921715] [0.30044925]\n",
      "100 [0.09978334] [0.30012435]\n",
      "120 [0.09994004] [0.30003443]\n",
      "140 [0.09998339] [0.30000955]\n",
      "160 [0.09999541] [0.30000266]\n",
      "180 [0.09999872] [0.30000076]\n",
      "200 [0.09999963] [0.30000022]\n"
     ]
    }
   ],
   "source": [
    "# TEST1: Linear Regression with TensorFlow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# create data\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "# create TensorFlow structure\n",
    "Weights = tf.Variable(tf.random.uniform([1], -1.0, 1.0))\n",
    "biases = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "def model(x):\n",
    "    return Weights * x + biases\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "\n",
    "# training loop\n",
    "for step in range(201):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_data)\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "    grads = tape.gradient(loss, [Weights, biases])\n",
    "    optimizer.apply_gradients(zip(grads, [Weights, biases]))\n",
    "    if step % 20 == 0:\n",
    "        print(step, Weights.numpy(), biases.numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Variable` 是 ***TensorFlow2*** 中表示可变张量（变量）的类。它是一种特殊的张量，与普通的张量不同，变量的值可以被修改。  \n",
    "通常情况下，使用变量来表示模型中的参数，例如神经网络中的权重和偏置项。\n",
    "\n",
    "与普通的张量不同，变量有以下特性：\n",
    "1. 变量的值可以被修改，且修改后会保持不变，直到再次被修改。\n",
    "2. 变量有一个初始值，在模型训练过程中，变量的值会不断被优化（更新）。\n",
    "3. 变量可以被保存和恢复。\n",
    "4. 变量可以在分布式环境下共享，实现模型的多卡训练和分布式训练。\n",
    "\n",
    "💊下面是一个简单的例子，演示如何创建一个变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 创建一个初始值为1.0的变量\n",
    "x = tf.Variable(1.0)\n",
    "\n",
    "# 将变量的值加1，并输出\n",
    "x.assign_add(1.0)\n",
    "print(x.numpy())  # Output: 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90000075\n",
      "0.8001683\n",
      "0.7006256\n",
      "0.6015081\n",
      "0.50295985\n",
      "0.4051363\n",
      "0.3082048\n",
      "0.21234292\n",
      "0.11773961\n",
      "0.024594113\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 创建一个初始值为1.0的变量\n",
    "x = tf.Variable(1.0)\n",
    "\n",
    "# 定义一个损失函数\n",
    "loss_fn = lambda: x**2 + 2*x + 1\n",
    "\n",
    "# 创建一个Adam优化器，并指定需要优化的变量\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "def train_step():\n",
    "    optimizer.minimize(loss_fn, var_list=[x])\n",
    "\n",
    "# 在模型的训练过程中不断更新变量的值\n",
    "for i in range(10):\n",
    "    train_step()\n",
    "    print(x.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.GradientTape` 是 ***TensorFlow2*** 中用于计算梯度的API之一。它可以记录张量的计算过程，并且可以自动计算梯度。\n",
    "\n",
    "使用tf.GradientTape的基本流程如下：  \n",
    "1. 创建一个tf.GradientTape对象，并且在这个对象的上下文环境中执行前向计算，将需要计算梯度的操作放在这个环境中。  \n",
    "2. 调用gradient()方法，计算张量相对于某些变量的梯度。  \n",
    "3. 对于需要优化的变量，使用优化器（如tf.keras.optimizers中的优化器）更新它们的值。\n",
    "\n",
    "📮下面是一个使用tf.GradientTape计算梯度的简单示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x**2 + 2*x + 1\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx) # Output: tf.Tensor(8.0, shape=(), dtype=float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
